name: Crawl Publishers

on:
  schedule:
    - cron: "0 */2 * * *"
  workflow_dispatch:
    inputs:
      url_limit:
        description: "Max URLs to crawl per run"
        required: false
        default: "800"
      concurrency:
        description: "Parallel workers"
        required: false
        default: "12"

concurrency:
  group: crawler-job-${{ github.ref_name }}
  cancel-in-progress: false

jobs:
  crawl:
    runs-on: ubuntu-latest
    timeout-minutes: 180
    defaults:
      run:
        working-directory: feedfocus-crawler
    env:
      MONGO_URI: ${{ secrets.MONGO_URI }}
      CRAWLER_URL_LIMIT: ${{ github.event.inputs.url_limit || '800' }}
      CRAWLER_PUBLISHER_CONCURRENCY: ${{ github.event.inputs.concurrency || '12' }}
      CRAWLER_FETCH_RETRIES: "4"
      CRAWLER_FETCH_TIMEOUT_MS: "15000"
      CRAWLER_ALLOWED_LANGUAGES: "en"
      CRAWLER_MONGO_CONNECT_RETRIES: "6"
      CRAWLER_MONGO_CONNECT_RETRY_DELAY_MS: "3000"
      CRAWLER_MAX_RUN_MINUTES: "165"
      CRAWLER_MAX_PUBLISHER_MINUTES: "20"
      CRAWLER_MAX_FUTURE_SKEW_MINUTES: "120"
      CRAWLER_MIN_RSS_CANDIDATES_FOR_SITEMAP: "80"
      NODE_OPTIONS: "--max-old-space-size=1536"

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Node
        uses: actions/setup-node@v4
        with:
          node-version: 20
          cache: npm
          cache-dependency-path: feedfocus-crawler/package-lock.json

      - name: Validate secrets
        run: |
          if [ -z "$MONGO_URI" ]; then
            echo "MONGO_URI secret is not configured"
            exit 1
          fi

      - name: Install dependencies
        run: npm ci

      - name: Validate crawler config
        run: |
          node --check src/crawl.js
          node -e "const fs=require('fs'); JSON.parse(fs.readFileSync('./data/publishers.json','utf8')); console.log('publishers.json valid')"

      - name: Run crawler with retry
        run: |
          set -e
          attempt=1
          max=2
          until [ "$attempt" -gt "$max" ]; do
            echo "Crawler run attempt ${attempt}/${max}"
            if node src/crawl.js; then
              exit 0
            fi
            if [ "$attempt" -lt "$max" ]; then
              echo "Retrying crawler in 20s..."
              sleep 20
            fi
            attempt=$((attempt + 1))
          done
          echo "Crawler failed after ${max} attempts"
          exit 1
