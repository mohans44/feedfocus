name: Crawl Publishers

on:
  schedule:
    - cron: "0 */2 * * *"
  workflow_dispatch:
    inputs:
      url_limit:
        description: "Max URLs to crawl per run"
        required: false
        default: "800"
      concurrency:
        description: "Parallel workers"
        required: false
        default: "12"

concurrency:
  group: crawler-job
  cancel-in-progress: false

jobs:
  crawl:
    runs-on: ubuntu-latest
    timeout-minutes: 55
    defaults:
      run:
        working-directory: feed-focus-crawler
    env:
      MONGO_URI: ${{ secrets.MONGO_URI }}
      CRAWLER_URL_LIMIT: ${{ github.event.inputs.url_limit || '800' }}
      CRAWLER_PUBLISHER_CONCURRENCY: ${{ github.event.inputs.concurrency || '12' }}

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Node
        uses: actions/setup-node@v4
        with:
          node-version: 20
          cache: npm
          cache-dependency-path: feed-focus-crawler/package-lock.json

      - name: Validate secrets
        run: |
          if [ -z "$MONGO_URI" ]; then
            echo "MONGO_URI secret is not configured"
            exit 1
          fi

      - name: Install dependencies
        run: npm ci

      - name: Run crawler
        run: node src/crawl.js
